{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
    "\n",
    "from pymystem3 import Mystem \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from catboost.text_processing import Tokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['then',\n",
       " 'its',\n",
       " 'an',\n",
       " 'y',\n",
       " 'they',\n",
       " 'off',\n",
       " 'it',\n",
       " 'to',\n",
       " 'most',\n",
       " 'own',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'why',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'being',\n",
       " 'was',\n",
       " 'during',\n",
       " 'their',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " 'shouldn',\n",
       " 'won',\n",
       " 'there',\n",
       " 'more',\n",
       " 'this',\n",
       " 'too',\n",
       " 'wasn',\n",
       " 'about',\n",
       " 'a',\n",
       " 'on',\n",
       " 're',\n",
       " 'up',\n",
       " 'be',\n",
       " 'myself',\n",
       " 'all',\n",
       " \"you'd\",\n",
       " 'he',\n",
       " 'some',\n",
       " 'can',\n",
       " 'will',\n",
       " 'yours',\n",
       " 'further',\n",
       " 'does',\n",
       " \"haven't\",\n",
       " 'been',\n",
       " 'below',\n",
       " \"shouldn't\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " 'that',\n",
       " 'those',\n",
       " 'how',\n",
       " 'didn',\n",
       " 'isn',\n",
       " 'our',\n",
       " \"mightn't\",\n",
       " 'where',\n",
       " 'such',\n",
       " 'i',\n",
       " 'his',\n",
       " \"you're\",\n",
       " 'ain',\n",
       " 'but',\n",
       " 'between',\n",
       " 'she',\n",
       " 'ourselves',\n",
       " 'of',\n",
       " 'nor',\n",
       " 'is',\n",
       " 'do',\n",
       " 'should',\n",
       " 'or',\n",
       " 'no',\n",
       " \"don't\",\n",
       " 'd',\n",
       " 'other',\n",
       " 'theirs',\n",
       " 'mightn',\n",
       " \"you've\",\n",
       " 'themselves',\n",
       " 'same',\n",
       " 'o',\n",
       " 'because',\n",
       " 'from',\n",
       " \"shan't\",\n",
       " 'has',\n",
       " 'did',\n",
       " 'through',\n",
       " 'again',\n",
       " 'the',\n",
       " 'until',\n",
       " 'them',\n",
       " 'any',\n",
       " \"hadn't\",\n",
       " 'just',\n",
       " \"doesn't\",\n",
       " 'you',\n",
       " 'll',\n",
       " 'if',\n",
       " \"isn't\",\n",
       " 'her',\n",
       " 'hasn',\n",
       " 'down',\n",
       " 'under',\n",
       " 'few',\n",
       " 'am',\n",
       " 'herself',\n",
       " 'your',\n",
       " \"wasn't\",\n",
       " 'm',\n",
       " 'out',\n",
       " 'here',\n",
       " 'me',\n",
       " 'himself',\n",
       " \"mustn't\",\n",
       " 'over',\n",
       " 'in',\n",
       " 'into',\n",
       " 'needn',\n",
       " 's',\n",
       " \"you'll\",\n",
       " 'for',\n",
       " 'haven',\n",
       " 'when',\n",
       " 'each',\n",
       " \"that'll\",\n",
       " 'which',\n",
       " 'my',\n",
       " 'ma',\n",
       " \"needn't\",\n",
       " 'while',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " 'than',\n",
       " 'after',\n",
       " 'having',\n",
       " 'both',\n",
       " 'yourselves',\n",
       " 'and',\n",
       " 'above',\n",
       " 'so',\n",
       " 't',\n",
       " 'now',\n",
       " 'once',\n",
       " 'who',\n",
       " 'we',\n",
       " 'mustn',\n",
       " 'don',\n",
       " 'very',\n",
       " 'as',\n",
       " 've',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'against',\n",
       " 'hadn',\n",
       " 'doing',\n",
       " \"wouldn't\",\n",
       " 'not',\n",
       " 'only',\n",
       " \"aren't\",\n",
       " 'yourself',\n",
       " \"couldn't\",\n",
       " 'are',\n",
       " 'hers',\n",
       " \"should've\",\n",
       " 'by',\n",
       " 'before',\n",
       " 'have',\n",
       " 'whom',\n",
       " 'ours',\n",
       " 'weren',\n",
       " 'him',\n",
       " 'shan',\n",
       " 'what',\n",
       " \"hasn't\",\n",
       " 'had',\n",
       " 'at',\n",
       " 'these']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = list(set(nltk_stopwords.words('english')))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv',index_col=0,encoding='utf-8')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from jayanthv thanx i dont know wat a admin does anyway but please dont block me jayanthv answer on\n",
      " if i agree wha what do i have to do with this \n",
      "  thepalace com domain article states thepalace com domain is available but was actually purchased s\n",
      " instead of turning r\n",
      " having discussed this matter at length i think wikipedia will benefit from an absence by me for sev\n",
      " adding fr to templates please have a look at this diff to see how you should have added fr to a tem\n",
      "  these quotes do not constitute evidence for the statement modern calculus has solved the mathemati\n",
      " jewish american don't worry madmax you did not drag me in i went voluntary unfortunately there are \n",
      " about the fake history of a fake marine\n",
      " redirect talk jeopardy video games \n",
      "CPU times: user 4.18 s, sys: 106 ms, total: 4.28 s\n",
      "Wall time: 4.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47570     None\n",
       "20081     None\n",
       "50844     None\n",
       "53034     None\n",
       "99896     None\n",
       "115764    None\n",
       "106675    None\n",
       "56782     None\n",
       "73204     None\n",
       "1203      None\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mst = Mystem()\n",
    "lst = LancasterStemmer()\n",
    "stemmer = lst.stem\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = ''.join( re.sub(r\"([^a-z\\'])+\",' ',text.lower()) )\n",
    "#     print(text)\n",
    "#     text = re.sub('( )+',' ',text)\n",
    "    return ''.join( stemmer(text)).strip('\\n')                \n",
    "\n",
    "# lemmatize(df.text[2])                    \n",
    "df['lemmas'] = df.text.apply(lemmatize)\n",
    "\n",
    "df.sample(10).lemmas.apply(lambda st: print(f\" {st:.99s}\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.62 ms, sys: 51 ms, total: 58.6 ms\n",
      "Wall time: 56.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tr,te = train_test_split(df, test_size = .25, shuffle = True)\n",
    "X = [ 'lemmas' ] \n",
    "y = 'toxic'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 s, sys: 1.58 s, total: 26.7 s\n",
      "Wall time: 26.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<119469x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3330140 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(stop_words = stopwords,max_features = 10_000,min_df=1/5_000,ngram_range=(1,3))\n",
    "vectors = tfidf.fit_transform(tr.lemmas)\n",
    "vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs = GridSearchCV(LogisticRegression(random_state=4999),\n",
    "                 {'max_iter':[2000],'class_weight':['balanced',None],'solver':['newton-cg', 'lbfgs', 'liblinear']},\n",
    "                  scoring = 'f1_micro',cv= 3)\n",
    "gs.fit(vectors,tr[y])\n",
    "lr = LogisticRegression( **gs.best_params_)\n",
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_vectors = tfidf.transform(te.lemmas)\n",
    "lr.fit(vectors,tr[y])\n",
    "pr = lr.predict(te_vectors)\n",
    "for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "    print( f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\" )\n",
    "\"confusion_matrix:\\n\",confusion_matrix(te[y],pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "del lr\n",
    "del gs\n",
    "time.sleep(5)\n",
    "gc.collect()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.3296584\ttotal: 5.91s\tremaining: 13h 27m 27s\n",
      "128:\tlearn: 0.6231289\ttotal: 1m 20s\tremaining: 1h 24m 20s\n",
      "256:\tlearn: 0.6803874\ttotal: 2m 47s\tremaining: 1h 26m 1s\n",
      "384:\tlearn: 0.7072470\ttotal: 4m 13s\tremaining: 1h 25m 36s\n",
      "512:\tlearn: 0.7257902\ttotal: 5m 31s\tremaining: 1h 22m 47s\n",
      "640:\tlearn: 0.7370985\ttotal: 6m 48s\tremaining: 1h 20m 13s\n",
      "768:\tlearn: 0.7473814\ttotal: 7m 58s\tremaining: 1h 16m 55s\n",
      "896:\tlearn: 0.7561617\ttotal: 9m 8s\tremaining: 1h 14m 18s\n",
      "1024:\tlearn: 0.7593365\ttotal: 10m 17s\tremaining: 1h 11m 57s\n",
      "1152:\tlearn: 0.7642716\ttotal: 11m 26s\tremaining: 1h 9m 52s\n",
      "1280:\tlearn: 0.7675067\ttotal: 12m 35s\tremaining: 1h 7m 55s\n",
      "1408:\tlearn: 0.7697397\ttotal: 13m 44s\tremaining: 1h 6m 7s\n",
      "1536:\tlearn: 0.7724731\ttotal: 14m 52s\tremaining: 1h 4m 25s\n",
      "1664:\tlearn: 0.7744210\ttotal: 16m 2s\tremaining: 1h 2m 52s\n",
      "1792:\tlearn: 0.7774822\ttotal: 17m 10s\tremaining: 1h 1m 18s\n",
      "1920:\tlearn: 0.7785420\ttotal: 18m 19s\tremaining: 59m 48s\n",
      "2048:\tlearn: 0.7805340\ttotal: 19m 28s\tremaining: 58m 21s\n",
      "2176:\tlearn: 0.7812766\ttotal: 20m 36s\tremaining: 56m 56s\n",
      "2304:\tlearn: 0.7823091\ttotal: 21m 45s\tremaining: 55m 33s\n",
      "2432:\tlearn: 0.7843174\ttotal: 22m 54s\tremaining: 54m 12s\n",
      "2560:\tlearn: 0.7865369\ttotal: 24m 2s\tremaining: 52m 51s\n",
      "2688:\tlearn: 0.7888143\ttotal: 25m 11s\tremaining: 51m 33s\n",
      "2816:\tlearn: 0.7895946\ttotal: 26m 20s\tremaining: 50m 14s\n",
      "2944:\tlearn: 0.7915165\ttotal: 27m 28s\tremaining: 48m 57s\n",
      "3072:\tlearn: 0.7931582\ttotal: 28m 37s\tremaining: 47m 40s\n",
      "3200:\tlearn: 0.7950919\ttotal: 29m 46s\tremaining: 46m 24s\n",
      "3328:\tlearn: 0.7969092\ttotal: 30m 54s\tremaining: 45m 9s\n",
      "3456:\tlearn: 0.7980845\ttotal: 32m 3s\tremaining: 43m 54s\n",
      "3584:\tlearn: 0.7992013\ttotal: 33m 12s\tremaining: 42m 39s\n",
      "3712:\tlearn: 0.7999258\ttotal: 34m 20s\tremaining: 41m 25s\n",
      "3840:\tlearn: 0.8018331\ttotal: 35m 29s\tremaining: 40m 11s\n",
      "3968:\tlearn: 0.8029238\ttotal: 36m 37s\tremaining: 38m 58s\n",
      "4096:\tlearn: 0.8044161\ttotal: 37m 46s\tremaining: 37m 45s\n",
      "4224:\tlearn: 0.8061851\ttotal: 38m 55s\tremaining: 36m 32s\n",
      "4352:\tlearn: 0.8069318\ttotal: 40m 3s\tremaining: 35m 20s\n",
      "4480:\tlearn: 0.8078446\ttotal: 41m 13s\tremaining: 34m 8s\n",
      "4608:\tlearn: 0.8091954\ttotal: 42m 22s\tremaining: 32m 56s\n",
      "4736:\tlearn: 0.8100731\ttotal: 43m 31s\tremaining: 31m 44s\n",
      "4864:\tlearn: 0.8112723\ttotal: 44m 39s\tremaining: 30m 32s\n",
      "4992:\tlearn: 0.8127695\ttotal: 45m 49s\tremaining: 29m 21s\n",
      "5120:\tlearn: 0.8130700\ttotal: 46m 58s\tremaining: 28m 9s\n",
      "5248:\tlearn: 0.8132875\ttotal: 48m 6s\tremaining: 26m 58s\n",
      "5376:\tlearn: 0.8146047\ttotal: 49m 14s\tremaining: 25m 46s\n",
      "5504:\tlearn: 0.8155429\ttotal: 50m 23s\tremaining: 24m 35s\n",
      "5632:\tlearn: 0.8171523\ttotal: 51m 32s\tremaining: 23m 24s\n",
      "5760:\tlearn: 0.8183063\ttotal: 52m 41s\tremaining: 22m 13s\n",
      "5888:\tlearn: 0.8186670\ttotal: 53m 49s\tremaining: 21m 3s\n",
      "6016:\tlearn: 0.8189860\ttotal: 54m 58s\tremaining: 19m 52s\n",
      "6144:\tlearn: 0.8202652\ttotal: 56m 7s\tremaining: 18m 41s\n",
      "6272:\tlearn: 0.8207861\ttotal: 57m 15s\tremaining: 17m 31s\n",
      "6400:\tlearn: 0.8218741\ttotal: 58m 24s\tremaining: 16m 20s\n",
      "6528:\tlearn: 0.8223193\ttotal: 59m 33s\tremaining: 15m 10s\n",
      "6656:\tlearn: 0.8225073\ttotal: 1h 41s\tremaining: 13m 59s\n",
      "6784:\tlearn: 0.8230909\ttotal: 1h 1m 50s\tremaining: 12m 49s\n",
      "6912:\tlearn: 0.8236256\ttotal: 1h 2m 59s\tremaining: 11m 39s\n",
      "7040:\tlearn: 0.8249932\ttotal: 1h 4m 7s\tremaining: 10m 28s\n",
      "7168:\tlearn: 0.8250681\ttotal: 1h 5m 16s\tremaining: 9m 18s\n",
      "7296:\tlearn: 0.8256173\ttotal: 1h 6m 24s\tremaining: 8m 8s\n",
      "7424:\tlearn: 0.8260712\ttotal: 1h 7m 32s\tremaining: 6m 58s\n",
      "7552:\tlearn: 0.8263750\ttotal: 1h 8m 41s\tremaining: 5m 48s\n",
      "7680:\tlearn: 0.8268542\ttotal: 1h 9m 49s\tremaining: 4m 38s\n",
      "7808:\tlearn: 0.8276519\ttotal: 1h 10m 58s\tremaining: 3m 28s\n",
      "7936:\tlearn: 0.8278176\ttotal: 1h 12m 7s\tremaining: 2m 19s\n",
      "8064:\tlearn: 0.8286582\ttotal: 1h 13m 16s\tremaining: 1m 9s\n",
      "8191:\tlearn: 0.8296196\ttotal: 1h 14m 24s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fb135636250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbi = CatBoostClassifier(learning_rate = .075,n_estimators =8192 ,\n",
    "                         bootstrap_type='Bernoulli',subsample=.2,\n",
    "                         verbose= 128, eval_metric='F1',random_state=499)\n",
    "cbi.fit(vectors,tr[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:0.7613\t\n",
      "accuracy_score:0.9576\t\n",
      "precision_score:0.8776\t\n",
      "recall_score:0.6722\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('confusion_matrix:\\n',\n",
       " array([[35438,   376],\n",
       "        [ 1314,  2695]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_vectors = tfidf.transform(te.lemmas)\n",
    "pr = cbi.predict(te_vectors)\n",
    "for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "    print( f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\" )\n",
    "\"confusion_matrix:\\n\",confusion_matrix(te[y],pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 -->\tf1_score:0.7603\taccuracy_score:0.9491\tprecision_score:0.7228\trecall_score:0.8019\t\n",
      "0.22 -->\tf1_score:0.7681\taccuracy_score:0.9519\tprecision_score:0.7464\trecall_score:0.7912\t\n",
      "0.25 -->\tf1_score:0.7732\taccuracy_score:0.954\tprecision_score:0.7683\trecall_score:0.7782\t\n",
      "0.28 -->\tf1_score:0.7727\taccuracy_score:0.9547\tprecision_score:0.7811\trecall_score:0.7645\t\n",
      "0.3 -->\tf1_score:0.7752\taccuracy_score:0.9559\tprecision_score:0.7958\trecall_score:0.7556\t\n",
      "0.32 -->\tf1_score:0.7757\taccuracy_score:0.9566\tprecision_score:0.8081\trecall_score:0.7458\t\n",
      "0.35 -->\tf1_score:0.7744\taccuracy_score:0.9569\tprecision_score:0.8182\trecall_score:0.7351\t\n",
      "0.38 -->\tf1_score:0.7735\taccuracy_score:0.9573\tprecision_score:0.8291\trecall_score:0.7249\t\n",
      "0.4 -->\tf1_score:0.7737\taccuracy_score:0.9579\tprecision_score:0.8423\trecall_score:0.7154\t\n",
      "0.42 -->\tf1_score:0.7723\taccuracy_score:0.9581\tprecision_score:0.8522\trecall_score:0.7062\t\n",
      "0.45 -->\tf1_score:0.7691\taccuracy_score:0.958\tprecision_score:0.8617\trecall_score:0.6944\t\n",
      "0.47 -->\tf1_score:0.7645\taccuracy_score:0.9577\tprecision_score:0.8686\trecall_score:0.6827\t\n",
      "0.5 -->\tf1_score:0.7613\taccuracy_score:0.9576\tprecision_score:0.8776\trecall_score:0.6722\t\n",
      "0.52 -->\tf1_score:0.7581\taccuracy_score:0.9575\tprecision_score:0.8881\trecall_score:0.6613\t\n",
      "0.55 -->\tf1_score:0.7525\taccuracy_score:0.957\tprecision_score:0.8937\trecall_score:0.6498\t\n",
      "0.57 -->\tf1_score:0.7461\taccuracy_score:0.9563\tprecision_score:0.8988\trecall_score:0.6378\t\n"
     ]
    }
   ],
   "source": [
    "for t in np.arange(.2,.6,.025):\n",
    "    cbi.set_probability_threshold(t)\n",
    "    pr = cbi.predict(te_vectors)\n",
    "    #confusion_matrix(te[y],pr)\n",
    "    \n",
    "    s = f\"{round(t,2)} -->\\t\"\n",
    "    for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "        s+=f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\"\n",
    "    print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2125\u001b[0m )\n\u001b[0;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:270\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_n, \u001b[38;5;28mmin\u001b[39m(max_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_original_tokens \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_original_tokens \u001b[38;5;241m-\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m             tokens_append(\u001b[43mspace_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(stop_words = stopwords,max_features = 10_000,min_df = 1/10_000,ngram_range=(1,3))\n",
    "vectors = tfidf.fit_transform(df.lemmas)\n",
    "vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_options =  {\n",
    "    \"tokenizers\" : [{\n",
    "        \"tokenizer_id\" : \"Space\",\n",
    "        \"delimiter\" : \" ,\",\n",
    "        \"lowercasing\" : \"true\",\n",
    "#        \"lemmatizing\" : \"true\",\n",
    "        \"split_by_set\": \"true\"                    \n",
    "    },{\n",
    "    'tokenizer_id': 'Sense',\n",
    "    'separator_type': 'BySense'\n",
    "    }],\n",
    "\n",
    "    \"dictionaries\" : [{\"dictionary_id\" : \"BiGram\",\n",
    "            \"max_dictionary_size\" : \"100000\",\n",
    "            \"occurrence_lower_bound\" : \"10\",\n",
    "            \"gram_order\" : \"2\"\n",
    "        },{\n",
    "        \"dictionary_id\" : \"Word\",\n",
    "        \"occurrence_lower_bound\" : \"20\",\n",
    "        \"gram_order\" : \"1\"\n",
    "    }],\n",
    "\n",
    "    \"feature_processing\" : {\n",
    "        \"default\" : [{\n",
    "            \"dictionaries_names\" : [\"Word\",\"BiGram\"],\n",
    "            \"feature_calcers\" : [\"BoW\"],\n",
    "            \"tokenizers_names\" : [\"Sense\"]\n",
    "        }]\n",
    "        \n",
    "    }\n",
    "}\n",
    "cbm = CatBoostClassifier(learning_rate = .075,n_estimators =8192 ,\n",
    "                         bootstrap_type='Bernoulli',subsample=.2,\n",
    "                         text_processing = text_options,\n",
    "                         verbose= 128, eval_metric='F1',random_state=499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tr = Pool(tr[X],tr[y], text_features = X ) \n",
    "p_te = Pool( te[X], text_features = X )\n",
    "cbm.fit(p_tr)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cbm.fit(tr[X],tr[y],verbose=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cbm.predict(p_te)\n",
    "confusion_matrix(te[y],pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Bad value for num_feature[non_default_doc_idx=0,feature_idx=0]=\" good luck at trying to turn this article into a good npov article when the cover to the book states that it was written by michael d langone and his colleagues from what i can see most of the notable authors belong to were associated with or associate themselves with the american family association as a matter of fact michael d langone phd is the executive director for the american family association which promotes controversial conservative fundamentalist christian values this indicates to me that this book and its authors are inherently biased the book appears to have been published to further the agenda of the american family association as it advertises it on the back cover of the book itself for example margaret singer was on the board of the afa talk email \": Cannot convert 'b' good luck at trying to turn this article into a good npov article when the cover to the book states that it was written by michael d langone and his colleagues from what i can see most of the notable authors belong to were associated with or associate themselves with the american family association as a matter of fact michael d langone phd is the executive director for the american family association which promotes controversial conservative fundamentalist christian values this indicates to me that this book and its authors are inherently biased the book appears to have been published to further the agenda of the american family association as it advertises it on the back cover of the book itself for example margaret singer was on the board of the afa talk email '' to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m_catboost.pyx:2288\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:1141\u001b[0m, in \u001b[0;36m_catboost._FloatOrNan\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:951\u001b[0m, in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert 'b' good luck at trying to turn this article into a good npov article when the cover to the book states that it was written by michael d langone and his colleagues from what i can see most of the notable authors belong to were associated with or associate themselves with the american family association as a matter of fact michael d langone phd is the executive director for the american family association which promotes controversial conservative fundamentalist christian values this indicates to me that this book and its authors are inherently biased the book appears to have been published to further the agenda of the american family association as it advertises it on the back cover of the book itself for example margaret singer was on the board of the afa talk email '' to float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m.2\u001b[39m,\u001b[38;5;241m.6\u001b[39m,\u001b[38;5;241m.025\u001b[39m):\n\u001b[1;32m      2\u001b[0m     cbi\u001b[38;5;241m.\u001b[39mset_probability_threshold(t)\n\u001b[0;32m----> 3\u001b[0m     pr \u001b[38;5;241m=\u001b[39m \u001b[43mcbi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mte\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#confusion_matrix(te[y],pr)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(t,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -->\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/catboost/core.py:5187\u001b[0m, in \u001b[0;36mCatBoostClassifier.predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, prediction_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, ntree_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ntree_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, thread_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   5137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5138\u001b[0m \u001b[38;5;124;03m    Predict with data.\u001b[39;00m\n\u001b[1;32m   5139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5185\u001b[0m \u001b[38;5;124;03m              with log probability for every class for each object.\u001b[39;00m\n\u001b[1;32m   5186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/catboost/core.py:2543\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2542\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2543\u001b[0m data, data_is_single_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_predict_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_type(prediction_type)\n\u001b[1;32m   2546\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/catboost/core.py:2523\u001b[0m, in \u001b[0;36mCatBoost._process_predict_input_data\u001b[0;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n\u001b[1;32m   2521\u001b[0m is_single_object \u001b[38;5;241m=\u001b[39m _is_data_single_object(data)\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Pool):\n\u001b[0;32m-> 2523\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_single_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cat_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthread_count\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, is_single_object\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/catboost/core.py:792\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             )\n\u001b[0;32m--> 792\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28msuper\u001b[39m(Pool, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/catboost/core.py:1419\u001b[0m, in \u001b[0;36mPool._init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1418\u001b[0m     feature_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_transform_tags(feature_tags, feature_names)\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_catboost.pyx:3955\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4005\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:3821\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:2787\u001b[0m, in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:2329\u001b[0m, in \u001b[0;36m_catboost.create_num_factor_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:2290\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=0]=\" good luck at trying to turn this article into a good npov article when the cover to the book states that it was written by michael d langone and his colleagues from what i can see most of the notable authors belong to were associated with or associate themselves with the american family association as a matter of fact michael d langone phd is the executive director for the american family association which promotes controversial conservative fundamentalist christian values this indicates to me that this book and its authors are inherently biased the book appears to have been published to further the agenda of the american family association as it advertises it on the back cover of the book itself for example margaret singer was on the board of the afa talk email \": Cannot convert 'b' good luck at trying to turn this article into a good npov article when the cover to the book states that it was written by michael d langone and his colleagues from what i can see most of the notable authors belong to were associated with or associate themselves with the american family association as a matter of fact michael d langone phd is the executive director for the american family association which promotes controversial conservative fundamentalist christian values this indicates to me that this book and its authors are inherently biased the book appears to have been published to further the agenda of the american family association as it advertises it on the back cover of the book itself for example margaret singer was on the board of the afa talk email '' to float"
     ]
    }
   ],
   "source": [
    "for t in np.arange(.2,.6,.025):\n",
    "    cbm.set_probability_threshold(t)\n",
    "    pr = cbm.predict(te[X])\n",
    "    #confusion_matrix(te[y],pr)\n",
    "    \n",
    "    s = f\"{round(t,2)} -->\\t\"\n",
    "    for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "        s+=f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\"\n",
    "    print(s)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbi.set_probability_threshold(.33)\n",
    "pr = cbi.predict(te[X])\n",
    "for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "    print( f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\" )\n",
    "\"confusion_matrix:\\n\",confusion_matrix(te[y],pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# params_grid ={\n",
    "#     'learning_rate':[0.3,.1,.03,.01],\n",
    "#     'auto_class_weights':['Balanced']\n",
    "# }\n",
    "# cbc = GridSearchCV( \n",
    "#     CatBoostClassifier( eval_metric='F1',text_features = X,tokenizers=verbose= 100  ),\n",
    "#     params_grid,\n",
    "#     scoring='f1_weighted',\n",
    "#     cv= 4)  \n",
    "# cbc.fit(tr[X],tr[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cbc.predict(te[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  CatBoostClassifier( eval_metric='F1',eval_period=50,text_features = X,verbose= 100,n_estimators=500 , learning_rate=.03 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
