{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import functools as fnc\n",
    "import operator as opr\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
    "\n",
    "from pymystem3 import Mystem \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from catboost.text_processing import Tokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(499)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка стопслов из NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nor',\n",
       " 'does',\n",
       " 'below',\n",
       " 'theirs',\n",
       " 'are',\n",
       " 'you',\n",
       " 'at',\n",
       " 'again',\n",
       " 'needn',\n",
       " 'over',\n",
       " \"you'll\",\n",
       " 'wasn',\n",
       " 'up',\n",
       " 'other',\n",
       " 'had',\n",
       " 'here',\n",
       " 'few',\n",
       " 'having',\n",
       " 's',\n",
       " 'shan',\n",
       " 'any',\n",
       " \"wasn't\",\n",
       " 'but',\n",
       " 'doing',\n",
       " 'was',\n",
       " 'aren',\n",
       " 'his',\n",
       " 'how',\n",
       " 'we',\n",
       " \"didn't\",\n",
       " \"wouldn't\",\n",
       " 'between',\n",
       " 'did',\n",
       " 'me',\n",
       " 'too',\n",
       " 'mustn',\n",
       " 'these',\n",
       " 'an',\n",
       " 'hasn',\n",
       " \"that'll\",\n",
       " 'll',\n",
       " \"should've\",\n",
       " 'he',\n",
       " 'only',\n",
       " 'o',\n",
       " 'through',\n",
       " 'i',\n",
       " 'in',\n",
       " 'him',\n",
       " 'has',\n",
       " 'then',\n",
       " 'can',\n",
       " 'were',\n",
       " 'for',\n",
       " 'under',\n",
       " 'be',\n",
       " 'who',\n",
       " 'out',\n",
       " 'our',\n",
       " 'no',\n",
       " 'hers',\n",
       " \"it's\",\n",
       " 'y',\n",
       " 'am',\n",
       " 'of',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'that',\n",
       " 'myself',\n",
       " 'been',\n",
       " 'those',\n",
       " 'with',\n",
       " \"mustn't\",\n",
       " 'now',\n",
       " 'ourselves',\n",
       " \"don't\",\n",
       " \"you're\",\n",
       " 'and',\n",
       " 'until',\n",
       " \"couldn't\",\n",
       " 'same',\n",
       " 'weren',\n",
       " 'they',\n",
       " 'their',\n",
       " 'if',\n",
       " 'by',\n",
       " 'or',\n",
       " 'while',\n",
       " 'them',\n",
       " 'will',\n",
       " 'wouldn',\n",
       " \"haven't\",\n",
       " 'my',\n",
       " 'themselves',\n",
       " 'during',\n",
       " 're',\n",
       " 'into',\n",
       " 'her',\n",
       " 'it',\n",
       " 'on',\n",
       " 'about',\n",
       " 'your',\n",
       " 'because',\n",
       " 'herself',\n",
       " \"shan't\",\n",
       " 'when',\n",
       " 'do',\n",
       " 'don',\n",
       " 'as',\n",
       " 'ma',\n",
       " 'should',\n",
       " 'just',\n",
       " 'further',\n",
       " \"you'd\",\n",
       " 'than',\n",
       " 'haven',\n",
       " 'from',\n",
       " 'yourself',\n",
       " 've',\n",
       " 'its',\n",
       " 'most',\n",
       " 'both',\n",
       " 'she',\n",
       " 'own',\n",
       " 'all',\n",
       " 'once',\n",
       " 'why',\n",
       " \"hasn't\",\n",
       " \"weren't\",\n",
       " 'being',\n",
       " 'mightn',\n",
       " 'each',\n",
       " 'whom',\n",
       " 'not',\n",
       " 'yourselves',\n",
       " 'ain',\n",
       " 'the',\n",
       " 'yours',\n",
       " 'what',\n",
       " 't',\n",
       " 'hadn',\n",
       " 'a',\n",
       " 'after',\n",
       " 'this',\n",
       " 'd',\n",
       " 'doesn',\n",
       " \"won't\",\n",
       " 'won',\n",
       " \"hadn't\",\n",
       " \"you've\",\n",
       " 'some',\n",
       " \"isn't\",\n",
       " 'such',\n",
       " 'which',\n",
       " 'didn',\n",
       " 'above',\n",
       " \"shouldn't\",\n",
       " 'before',\n",
       " \"she's\",\n",
       " \"doesn't\",\n",
       " 'shouldn',\n",
       " \"needn't\",\n",
       " 'more',\n",
       " 'ours',\n",
       " 'to',\n",
       " \"aren't\",\n",
       " 'itself',\n",
       " 'where',\n",
       " 'couldn',\n",
       " 'there',\n",
       " 'against',\n",
       " 'm',\n",
       " \"mightn't\",\n",
       " 'very',\n",
       " 'have',\n",
       " 'down',\n",
       " 'off',\n",
       " 'isn',\n",
       " 'so']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = list(set(nltk_stopwords.words('english')))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv',index_col=0,encoding='utf-8')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Mystem.__del__ at 0x7f75ac277d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ltz/anaconda3/lib/python3.9/site-packages/pymystem3/mystem.py\", line 196, in __del__\n",
      "    self.close()  # terminate process on exit\n",
      "  File \"/home/ltz/anaconda3/lib/python3.9/site-packages/pymystem3/mystem.py\", line 216, in close\n",
      "    if self._proc is not None:\n",
      "AttributeError: 'Mystem' object has no attribute '_proc'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'language'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mst = Mystem()\n",
    "lst = LancasterStemmer()\n",
    "snb = SnowballStemmer('english')\n",
    "stemmer = lst.stem\n",
    "\n",
    "def lemmatize(text,stemmer):\n",
    "    pure = re.sub(r\"([^a-z\\'])+\",' ',text.lower())\n",
    "    words =  pure.split()  \n",
    "    return( list( map(stemmer,words)) )\n",
    "#     print(text)\n",
    "#     text = re.sub('( )+',' ',text)\n",
    "#    return ''.join( stemmer(text)).strip('\\n')                \n",
    "def lanc_stem(text):\n",
    "    return lemmatize(text,lst.stem)\n",
    "def snow_stem(text):\n",
    "    return lemmatize(text,snb.stem)\n",
    "\n",
    "def my_stem(text):\n",
    "    return mst.lemmatize(text)\n",
    "# lanc_stem.__name__='lancaster'\n",
    "# my_stem.__name__ = 'mystem'\n",
    "\n",
    "print( lanc_stem('a  lot of   matches') )\n",
    "print( snow_stem('a  lot of   matches') )\n",
    "print( my_stem('matches') )\n",
    "# lemmatize(df.text[2])                    \n",
    "#df['lemmas'] = df.text.apply(lemmatize)\n",
    "\n",
    "# def lemmatizer(serie):\n",
    "#     return serie.apply(lemmatize)\n",
    "# #df.sample(10).lemmas.apply(lambda st: print(f\" {st:.99s}\") )\n",
    "\n",
    "# lemmatizer(df.text.head(20))\n",
    "\n",
    "# TfidfVectorizer(tokenizer=lemmatize).fit_transform(df.text.head(20)).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение на тренировочный и тестовый наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.8 ms, sys: 0 ns, total: 68.8 ms\n",
      "Wall time: 66.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = 'text'  \n",
    "y = 'toxic'\n",
    "\n",
    "tr,te = train_test_split(df, test_size = .25, shuffle = True, stratify=df[y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вспомогательные классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([38.17715627, 30.66127861, 35.12304491, 28.29321259, 36.44178987,\n",
       "        29.09740239, 35.34423548, 28.3945964 ]),\n",
       " 'std_fit_time': array([0.52054316, 1.43671587, 0.27165651, 0.11460409, 0.44121281,\n",
       "        0.26119635, 0.14528297, 0.16966969]),\n",
       " 'mean_score_time': array([12.71362698,  9.60080045, 11.492836  ,  9.17770272, 11.8043955 ,\n",
       "         9.32051241, 11.59718955,  9.16020846]),\n",
       " 'std_score_time': array([0.58186459, 0.52616134, 0.18117816, 0.06713648, 0.20182173,\n",
       "        0.11155441, 0.14544406, 0.08977537]),\n",
       " 'param_model__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs'],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__max_features': masked_array(data=[3000, 3000, 3000, 3000, 4000, 4000, 4000, 4000],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),\n",
       "                    (1, 1)],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__stop_words': masked_array(data=[list([' ']), list([' ']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list([' ']), list([' ']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so'])],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__tokenizer': masked_array(data=[<function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>}],\n",
       " 'split0_test_score': array([0.75136399, 0.75297329, 0.74307992, 0.73895109, 0.75738725,\n",
       "        0.75209674, 0.74707716, 0.74568289]),\n",
       " 'split1_test_score': array([0.74094273, 0.73893547, 0.74069722, 0.73837667, 0.74111675,\n",
       "        0.73928293, 0.74043555, 0.73823124]),\n",
       " 'split2_test_score': array([0.74501369, 0.74433151, 0.74308145, 0.74433151, 0.74429583,\n",
       "        0.74814235, 0.73941304, 0.74577603]),\n",
       " 'split3_test_score': array([0.75642787, 0.75744514, 0.74617497, 0.75259549, 0.75657636,\n",
       "        0.75902669, 0.74803459, 0.7501472 ]),\n",
       " 'mean_test_score': array([0.74843707, 0.74842135, 0.74325839, 0.74356369, 0.74984405,\n",
       "        0.74963718, 0.74374009, 0.74495934]),\n",
       " 'std_test_score': array([0.00592251, 0.00722596, 0.00194481, 0.00570842, 0.0072314 ,\n",
       "        0.00713543, 0.0038478 , 0.00428287]),\n",
       " 'rank_test_score': array([3, 4, 8, 7, 1, 2, 6, 5], dtype=int32)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Классы для преобразваний результата кроссвалидации в датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedResult:\n",
    "    def __init__(self,result,metric_name='score',sample_name='mean'):\n",
    "        self.df = pd.DataFrame.from_dict( \n",
    "            { k:v for k,v in result.items() if k not in ['params']}\n",
    "        )\n",
    "# drop param_ prefix from column names for tidy display         \n",
    "        self.primary_score=f\"{sample_name}_test_{metric_name}\"\n",
    "        param_names = [ c for c in self.df.columns if c[:6]=='param_' ]        \n",
    "        self.param_cols = [ c[6:] for c in param_names ]              \n",
    "        self.df = self.df.rename( columns={ p:c for p,c in zip(param_names,self.param_cols) } )\n",
    "# also drop model_ prefix which we use for model grid in pipeline   \n",
    "        model_names = [ c for c in self.df.columns if c[:7]=='model__' ]\n",
    "        self.param_cols = [c[7:]  if c[:7]=='model__' else c for c in self.param_cols ]\n",
    "        self.model_cols = [ c[7:] for c in model_names ]   \n",
    "        self.df = self.df.rename( columns={ p:c for p,c in zip(model_names,self.model_cols) } )\n",
    "# some object columns may be not string (f.e. lists)\n",
    "        for i in range(len(self.df.columns)):\n",
    "            if self.df.dtypes[i]=='object':\n",
    "                self.df[self.df.columns[i]] = self.df[self.df.columns[i]].astype(str)\n",
    "# select parameters, for which tested several values and arrange them descending (by number of differnt values)\n",
    "        values_per_cols = list(zip( self.param_cols,[ len( np.unique( self.df[c].values ) ) for c in self.param_cols]))\n",
    "        self.multi_value_cols =[it[0] for it in sorted(values_per_cols, key=lambda pair:pair[1],reverse=True) if it[1]>1 ]\n",
    "\n",
    "        \n",
    "            \n",
    "    def select(self, index,values=[],filters={},agg=[] ):\n",
    "        values = [ [self.primary_score] ,values][bool(values)]\n",
    "        if len(filters)>0 :\n",
    "            condition = 'and'.join([ f\" {k}=={v} \" for k,v in filters.items() ]) \n",
    "            filtered = self.df.query(condition)    \n",
    "        else:\n",
    "            filtered = self.df\n",
    "        cols = [ c for c in self.multi_value_cols if c not in list(filters.keys())+[index]+values ]      \n",
    "        return(self.df.pivot_table(index=index,values=values,columns=cols)  )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Классы для презентации графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Styler:\n",
    "    base_array=[]\n",
    "    def __init__(self,param_array=[]):\n",
    "        self.params = [ self.base_array,param_array][bool(param_array)]\n",
    "    \n",
    "    def key(self):\n",
    "        pass\n",
    "    \n",
    "    def put(self,dct,index):\n",
    "        dct[self.key()] = self.params[index]\n",
    "        \n",
    "class ColorStyler(Styler):\n",
    "    base_array=[*'rgbykm']\n",
    "    def key(self):\n",
    "        return('color')\n",
    "\n",
    "class DashStyler(Styler):\n",
    "    base_array=['--',':','-.','-']\n",
    "    def key(self):\n",
    "        return('ls')\n",
    "    \n",
    "class WidthStyler(Styler):\n",
    "    base_array=[ 2,4,7]\n",
    "    def key(self):\n",
    "        return('lw')\n",
    "\n",
    "class AlphasStyler(Styler):\n",
    "    base_array=[ .3,.9]\n",
    "    def key(self):\n",
    "        return('alpha')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultPlotter:\n",
    "    base_params = {\n",
    "        'colors':[*'rgbykm'],\n",
    "        'styles':['--',':','-.','-',(0, (1, 10)),(0, (5, 10))],\n",
    "        'widths':[ 2,4,7],\n",
    "        'alphas':[.3,.8],\n",
    "        'figsize': (20,8),\n",
    "        'logscale':\"\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, test, params = {} ):\n",
    "        self.model = test.model\n",
    "        self.df= test.sel\n",
    "        self.params=self.base_params | params\n",
    "        self.stylers = [ColorStyler(self.params['colors']),DashStyler(self.params['styles']),\n",
    "                        WidthStyler(self.params['widths']),AlphasStyler(self.params['alphas'])]\n",
    "        self.mcols = self.df.columns      \n",
    "        self.lev_names = list(self.mcols.names)\n",
    "        self.num_levels= len(self.lev_names)\n",
    "        self.level_sizes=[ len(set(self.mcols.get_level_values(x))) for x in self.lev_names]\n",
    "        \n",
    "        # if level 0(\"values of pivot\") is one-valued, then start apply styles from columns 1  \n",
    "        self.start_level = int(  self.level_sizes[0] == 1   )\n",
    "# TODO: refactoring needed here, one or two new functions to encapsulate this feature\n",
    "#       also to print short description for \" result thread\" \n",
    "\n",
    "        \n",
    "\n",
    "# auxliary func to forms sequence of indexes for levels\n",
    "# result is need index on the level  which may be one of color|dashstyle|linewidth\n",
    "    def idx_on_level(self,idx, lev_num):\n",
    "        items_on_hyperplane = multiply(self.level_sizes[(lev_num+1):]) \n",
    "        return idx//items_on_hyperplane% self.level_sizes[lev_num]\n",
    "    \n",
    "    def plot(self):\n",
    "        fig,ax = plt.subplots(figsize=(20,6))\n",
    "        if 'x' in self.params['logscale'].lower():\n",
    "            ax.set_xscale('log')\n",
    "        if not self.lev_names[0]:\n",
    "            self.lev_names[0]='res.thread'\n",
    "            \n",
    "        for i_mcol in range(len( self.mcols)):\n",
    "            graph_params = {}\n",
    "            for i_level in range(self.start_level,self.num_levels):\n",
    "                self.stylers[i_level-self.start_level].put( graph_params, self.idx_on_level(i_mcol,i_level) ) \n",
    "            idx=self.mcols[i_mcol]\n",
    "            \n",
    "            levels_for_label=[ f\"{n}:{j} \" for n,j in zip(self.lev_names[self.start_level:],idx[self.start_level:]) ] \n",
    "            graph_params['label'] = ','.join( levels_for_label )\n",
    "            ax.plot(self.df[idx] ,**graph_params)\n",
    "            ax.set_title(f\"Тест {self.params['title']}\")\n",
    "            ax.set_xlabel('learning rate')\n",
    "            ax.set_ylabel(f\"{self.params['metric']}\")\n",
    "            ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подбор параметров словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " В принципе, для пользовния языком достаточно 4-5К слов , больший активный словарь уже признак хорошего литературного языка и такой объем вряд ли используется в коротких текстах.    \n",
    " Понятно, что чем больше словарь  -  тем точнее, но тест  ограничен ресурсами компьютера и временем, так что я хочу подобрать такой размер словаря , расширение которого  уже не дает существенного улучшения результата.   \n",
    " Я предполагаю, что потеря метрики от словаря мало зависит от модели , поэтому для скорости использую LogisticRegression.   \n",
    " Здесь же проверяется эффект от добавления в словарь n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'aft', 'al', 'ar', 'becaus', 'bef', 'doe', 'dur', 'furth', 'hav', 'mor', 'ont', 'oth', 'ourselv', 'ov', 'sam', 'som', 'themselv', 'ther', 'thes', 'thi', 'thos', 'und', 'wer', 'wher', 'whil', 'wil', 'yo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/ltz/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 53s, sys: 949 ms, total: 23min 54s\n",
      "Wall time: 23min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([38.17715627, 30.66127861, 35.12304491, 28.29321259, 36.44178987,\n",
       "        29.09740239, 35.34423548, 28.3945964 ]),\n",
       " 'std_fit_time': array([0.52054316, 1.43671587, 0.27165651, 0.11460409, 0.44121281,\n",
       "        0.26119635, 0.14528297, 0.16966969]),\n",
       " 'mean_score_time': array([12.71362698,  9.60080045, 11.492836  ,  9.17770272, 11.8043955 ,\n",
       "         9.32051241, 11.59718955,  9.16020846]),\n",
       " 'std_score_time': array([0.58186459, 0.52616134, 0.18117816, 0.06713648, 0.20182173,\n",
       "        0.11155441, 0.14544406, 0.08977537]),\n",
       " 'param_model__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs'],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__max_features': masked_array(data=[3000, 3000, 3000, 3000, 4000, 4000, 4000, 4000],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),\n",
       "                    (1, 1)],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__stop_words': masked_array(data=[list([' ']), list([' ']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list([' ']), list([' ']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']),\n",
       "                    list(['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so'])],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__tokenizer': masked_array(data=[<function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>,\n",
       "                    <function lanc_stem at 0x7f7572be7ca0>,\n",
       "                    <function snow_stem at 0x7f7572b61820>],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 3000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': [' '],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.lanc_stem(text)>},\n",
       "  {'model__solver': 'lbfgs',\n",
       "   'vect__max_features': 4000,\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['nor',\n",
       "    'does',\n",
       "    'below',\n",
       "    'theirs',\n",
       "    'are',\n",
       "    'you',\n",
       "    'at',\n",
       "    'again',\n",
       "    'needn',\n",
       "    'over',\n",
       "    \"you'll\",\n",
       "    'wasn',\n",
       "    'up',\n",
       "    'other',\n",
       "    'had',\n",
       "    'here',\n",
       "    'few',\n",
       "    'having',\n",
       "    's',\n",
       "    'shan',\n",
       "    'any',\n",
       "    \"wasn't\",\n",
       "    'but',\n",
       "    'doing',\n",
       "    'was',\n",
       "    'aren',\n",
       "    'his',\n",
       "    'how',\n",
       "    'we',\n",
       "    \"didn't\",\n",
       "    \"wouldn't\",\n",
       "    'between',\n",
       "    'did',\n",
       "    'me',\n",
       "    'too',\n",
       "    'mustn',\n",
       "    'these',\n",
       "    'an',\n",
       "    'hasn',\n",
       "    \"that'll\",\n",
       "    'll',\n",
       "    \"should've\",\n",
       "    'he',\n",
       "    'only',\n",
       "    'o',\n",
       "    'through',\n",
       "    'i',\n",
       "    'in',\n",
       "    'him',\n",
       "    'has',\n",
       "    'then',\n",
       "    'can',\n",
       "    'were',\n",
       "    'for',\n",
       "    'under',\n",
       "    'be',\n",
       "    'who',\n",
       "    'out',\n",
       "    'our',\n",
       "    'no',\n",
       "    'hers',\n",
       "    \"it's\",\n",
       "    'y',\n",
       "    'am',\n",
       "    'of',\n",
       "    'himself',\n",
       "    'is',\n",
       "    'that',\n",
       "    'myself',\n",
       "    'been',\n",
       "    'those',\n",
       "    'with',\n",
       "    \"mustn't\",\n",
       "    'now',\n",
       "    'ourselves',\n",
       "    \"don't\",\n",
       "    \"you're\",\n",
       "    'and',\n",
       "    'until',\n",
       "    \"couldn't\",\n",
       "    'same',\n",
       "    'weren',\n",
       "    'they',\n",
       "    'their',\n",
       "    'if',\n",
       "    'by',\n",
       "    'or',\n",
       "    'while',\n",
       "    'them',\n",
       "    'will',\n",
       "    'wouldn',\n",
       "    \"haven't\",\n",
       "    'my',\n",
       "    'themselves',\n",
       "    'during',\n",
       "    're',\n",
       "    'into',\n",
       "    'her',\n",
       "    'it',\n",
       "    'on',\n",
       "    'about',\n",
       "    'your',\n",
       "    'because',\n",
       "    'herself',\n",
       "    \"shan't\",\n",
       "    'when',\n",
       "    'do',\n",
       "    'don',\n",
       "    'as',\n",
       "    'ma',\n",
       "    'should',\n",
       "    'just',\n",
       "    'further',\n",
       "    \"you'd\",\n",
       "    'than',\n",
       "    'haven',\n",
       "    'from',\n",
       "    'yourself',\n",
       "    've',\n",
       "    'its',\n",
       "    'most',\n",
       "    'both',\n",
       "    'she',\n",
       "    'own',\n",
       "    'all',\n",
       "    'once',\n",
       "    'why',\n",
       "    \"hasn't\",\n",
       "    \"weren't\",\n",
       "    'being',\n",
       "    'mightn',\n",
       "    'each',\n",
       "    'whom',\n",
       "    'not',\n",
       "    'yourselves',\n",
       "    'ain',\n",
       "    'the',\n",
       "    'yours',\n",
       "    'what',\n",
       "    't',\n",
       "    'hadn',\n",
       "    'a',\n",
       "    'after',\n",
       "    'this',\n",
       "    'd',\n",
       "    'doesn',\n",
       "    \"won't\",\n",
       "    'won',\n",
       "    \"hadn't\",\n",
       "    \"you've\",\n",
       "    'some',\n",
       "    \"isn't\",\n",
       "    'such',\n",
       "    'which',\n",
       "    'didn',\n",
       "    'above',\n",
       "    \"shouldn't\",\n",
       "    'before',\n",
       "    \"she's\",\n",
       "    \"doesn't\",\n",
       "    'shouldn',\n",
       "    \"needn't\",\n",
       "    'more',\n",
       "    'ours',\n",
       "    'to',\n",
       "    \"aren't\",\n",
       "    'itself',\n",
       "    'where',\n",
       "    'couldn',\n",
       "    'there',\n",
       "    'against',\n",
       "    'm',\n",
       "    \"mightn't\",\n",
       "    'very',\n",
       "    'have',\n",
       "    'down',\n",
       "    'off',\n",
       "    'isn',\n",
       "    'so'],\n",
       "   'vect__tokenizer': <function __main__.snow_stem(text)>}],\n",
       " 'split0_test_score': array([0.75136399, 0.75297329, 0.74307992, 0.73895109, 0.75738725,\n",
       "        0.75209674, 0.74707716, 0.74568289]),\n",
       " 'split1_test_score': array([0.74094273, 0.73893547, 0.74069722, 0.73837667, 0.74111675,\n",
       "        0.73928293, 0.74043555, 0.73823124]),\n",
       " 'split2_test_score': array([0.74501369, 0.74433151, 0.74308145, 0.74433151, 0.74429583,\n",
       "        0.74814235, 0.73941304, 0.74577603]),\n",
       " 'split3_test_score': array([0.75642787, 0.75744514, 0.74617497, 0.75259549, 0.75657636,\n",
       "        0.75902669, 0.74803459, 0.7501472 ]),\n",
       " 'mean_test_score': array([0.74843707, 0.74842135, 0.74325839, 0.74356369, 0.74984405,\n",
       "        0.74963718, 0.74374009, 0.74495934]),\n",
       " 'std_test_score': array([0.00592251, 0.00722596, 0.00194481, 0.00570842, 0.0072314 ,\n",
       "        0.00713543, 0.0038478 , 0.00428287]),\n",
       " 'rank_test_score': array([3, 4, 8, 7, 1, 2, 6, 5], dtype=int32)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe = Pipeline(steps = [\n",
    "    ['vect',  TfidfVectorizer(tokenizer=lemmatize,stop_words = stopwords)],\n",
    "    ['model', LogisticRegression(max_iter= 2000)]\n",
    "])\n",
    "params_grid = {\n",
    "    'model__solver': ['lbfgs'],\n",
    "    'vect__max_features':[3_000,4_000],#,5000,6_000,7_000,8_000,9_000],\n",
    "#    'vect__min_df' : [1/5_000,1/10_000],\n",
    "    'vect__tokenizer': [lanc_stem,snow_stem],\n",
    "    'vect__stop_words': [ [' '],stopwords],    \n",
    "    'vect__ngram_range':[(1,1)]#,(1,2),(1,3),(1,4)]\n",
    "}\n",
    "gs=GridSearchCV(pipe, params_grid,\n",
    "                cv=4 ,\n",
    "                scoring = 'f1')\n",
    "gs.fit(tr[X].values,tr[y])\n",
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">mean_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vect__stop_words</th>\n",
       "      <th colspan=\"2\" halign=\"left\">[' ']</th>\n",
       "      <th colspan=\"2\" halign=\"left\">['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vect__tokenizer</th>\n",
       "      <th>&lt;function lanc_stem at 0x7f7572be7ca0&gt;</th>\n",
       "      <th>&lt;function snow_stem at 0x7f7572b61820&gt;</th>\n",
       "      <th>&lt;function lanc_stem at 0x7f7572be7ca0&gt;</th>\n",
       "      <th>&lt;function snow_stem at 0x7f7572b61820&gt;</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vect__max_features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.748437</td>\n",
       "      <td>0.748421</td>\n",
       "      <td>0.743258</td>\n",
       "      <td>0.743564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.749844</td>\n",
       "      <td>0.749637</td>\n",
       "      <td>0.743740</td>\n",
       "      <td>0.744959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          mean_test_score  \\\n",
       "vect__stop_words                                    [' ']   \n",
       "vect__tokenizer    <function lanc_stem at 0x7f7572be7ca0>   \n",
       "vect__max_features                                          \n",
       "3000                                             0.748437   \n",
       "4000                                             0.749844   \n",
       "\n",
       "                                                           \\\n",
       "vect__stop_words                                            \n",
       "vect__tokenizer    <function snow_stem at 0x7f7572b61820>   \n",
       "vect__max_features                                          \n",
       "3000                                             0.748421   \n",
       "4000                                             0.749637   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\\n",
       "vect__stop_words   ['nor', 'does', 'below', 'theirs', 'are', 'you', 'at', 'again', 'needn', 'over', \"you'll\", 'wasn', 'up', 'other', 'had', 'here', 'few', 'having', 's', 'shan', 'any', \"wasn't\", 'but', 'doing', 'was', 'aren', 'his', 'how', 'we', \"didn't\", \"wouldn't\", 'between', 'did', 'me', 'too', 'mustn', 'these', 'an', 'hasn', \"that'll\", 'll', \"should've\", 'he', 'only', 'o', 'through', 'i', 'in', 'him', 'has', 'then', 'can', 'were', 'for', 'under', 'be', 'who', 'out', 'our', 'no', 'hers', \"it's\", 'y', 'am', 'of', 'himself', 'is', 'that', 'myself', 'been', 'those', 'with', \"mustn't\", 'now', 'ourselves', \"don't\", \"you're\", 'and', 'until', \"couldn't\", 'same', 'weren', 'they', 'their', 'if', 'by', 'or', 'while', 'them', 'will', 'wouldn', \"haven't\", 'my', 'themselves', 'during', 're', 'into', 'her', 'it', 'on', 'about', 'your', 'because', 'herself', \"shan't\", 'when', 'do', 'don', 'as', 'ma', 'should', 'just', 'further', \"you'd\", 'than', 'haven', 'from', 'yourself', 've', 'its', 'most', 'both', 'she', 'own', 'all', 'once', 'why', \"hasn't\", \"weren't\", 'being', 'mightn', 'each', 'whom', 'not', 'yourselves', 'ain', 'the', 'yours', 'what', 't', 'hadn', 'a', 'after', 'this', 'd', 'doesn', \"won't\", 'won', \"hadn't\", \"you've\", 'some', \"isn't\", 'such', 'which', 'didn', 'above', \"shouldn't\", 'before', \"she's\", \"doesn't\", 'shouldn', \"needn't\", 'more', 'ours', 'to', \"aren't\", 'itself', 'where', 'couldn', 'there', 'against', 'm', \"mightn't\", 'very', 'have', 'down', 'off', 'isn', 'so']   \n",
       "vect__tokenizer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <function lanc_stem at 0x7f7572be7ca0>   \n",
       "vect__max_features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3000                                                         0.743258                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4000                                                         0.743740                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "\n",
       "                                                           \n",
       "vect__stop_words                                           \n",
       "vect__tokenizer    <function snow_stem at 0x7f7572b61820>  \n",
       "vect__max_features                                         \n",
       "3000                                             0.743564  \n",
       "4000                                             0.744959  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "multiply = lambda array: fnc.reduce( opr.mul,array,1 )\n",
    "p = ParsedResult(gs.cv_results_)\n",
    "sel = p.select('vect__max_features')\n",
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75556/1080313860.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mResultPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_75556/3231493726.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, test, params)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_params\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         self.stylers = [ColorStyler(self.params['colors']),DashStyler(self.params['styles']),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "ResultPlotter(sel).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширение словаря за 6000 слов не дает существенного прироста метрики,\n",
    "Несколько неожиданно , что учёт сочетаний слов ухудшает метрику, и чем длиннее сочетания, тем хуже.\n",
    "Дальше вся работа идет со словарем 7_000 слов и  n-граммами (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подбор параметров модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсчет модели занимает довольно много времени, слеудющий блок выполняется почти 3 часа.  \n",
    "Для ускорения расчет производится на субсэмплах.\n",
    "Подбирается оптимальная скорость обучения и способ балансировки для 1024 оценщиков.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe = Pipeline(steps = [\n",
    "    ['vect',  TfidfVectorizer(stop_words = stopwords, max_features=7_000, ngram_range=(1,1))],\n",
    "    ['model', CatBoostClassifier(\n",
    "                n_estimators = 1024,\n",
    "                bootstrap_type='Bernoulli',subsample=.1,\n",
    "                eval_metric='F1',verbose =128) ]\n",
    "])\n",
    "params_grid = {    \n",
    "    'model__learning_rate':[.05,.1,.2,.5],\n",
    "    'model__auto_class_weights':['None','Balanced','SqrtBalanced']\n",
    "}\n",
    "gs=GridSearchCV(pipe, params_grid,\n",
    "                cv=3 ,\n",
    "                scoring = 'f1')\n",
    "gs.fit(tr[X].values,tr[y])\n",
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiply = lambda array: fnc.reduce( opr.mul,array,1 )\n",
    "p = ParsedResult(gs.cv_results_)\n",
    "sel = p.select('learning_rate')\n",
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultPlotter(sel).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для финального теста выбран лучший результат - баланс SqrtBalance, скорость обучения .2,   \n",
    "  она попадет в целевую зону по результатам кроссвалидации.    \n",
    "Возможно модель без баланса недостаточно обучена  и из неё можно выжать больше, но проверка обучения займёт слишком много времени "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Финальный тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words = stopwords, max_features=7_000, ngram_range=(1,1))\n",
    "cb = CatBoostClassifier(n_estimators = 1024,\n",
    "#                bootstrap_type='Bernoulli',subsample=.1,\n",
    "                    learning_rate=.2,auto_class_weights='SqrtBalanced',\n",
    "                   random_state=4999,\n",
    "                   eval_metric='F1',verbose =128)\n",
    "pipe = Pipeline( [['vect',tf], ['model',cb] ] )\n",
    "pipe.fit(tr[X],tr[y])\n",
    "\n",
    "pr = pipe.predict(te[X])\n",
    "for metric in [f1_score,accuracy_score,precision_score,recall_score]:\n",
    "    print( f\"{metric.__name__}:{round( metric(te[y],pr), 4)}\\t\" )\n",
    "confusion_matrix(te[y],pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При финальном тесте достигнуто значительное улучшение метрики - <b>0.788</b>  \n",
    "Причиной улучшение я считаю использованипе полного набора данных при финальном тестировании "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
